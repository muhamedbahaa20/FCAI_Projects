# import numpy as np
# import matplotlib.pyplot as plt
# import math
#
# import pandas as pd
# import seaborn as sns
# import pandas
#
#
# # # x_train is the input variable (size in 1000 square feet)
# # # y_train is the target (price in 1000s of dollars)
# data =pd.read_csv("data.csv")
# x_train = data.iloc[:,:-1].values
# y_train = data.iloc[:,:-1].values
# # print(f"x_train = {x_train}")
# # print(f"y_train = {y_train}")
# # # m is the number of training examples
#
# # print(f"x_train.shape: {x_train.shape}")
# m = x_train.shape[0]
# # print(f"Number of training examples is: {m}")
# for i in range(m): # Change this to 1 to see (x^1, y^1)
#     x_i = x_train[i]
#     y_i = y_train[i]
# # print(f"(x^({i}), y^({i})) = ({x_i}, {y_i})")
# # # Plot the data points
#
# #
# w = 200
# b = 100
# # print(f"w: {w}")
# # print(f"b: {b}")
# #
# #
# # def compute_model_output(x, w, b):
# """
# Computes the prediction of a linear model
# Args:
#   x (ndarray (m,)): Data, m examples
#   w,b (scalar)    : model parameters
# Returns
#   f_wb (ndarray (m,)): model prediction
# """
# m = x_i.shape[0]
# f_wb = np.zeros(m)
# for i in range(m):
#     f_wb[i] = w * x_i[i] + b
#
# # return f_wb
#
# plt.scatter(x_train, y_train, marker='x', c='r')
# # Set the title
# plt.title("Housing Prices")
# # Set the y-axis label
# plt.ylabel('Price (in 1000s of dollars)')
# # Set the x-axis label
# plt.xlabel('Size (1000 sqft)')
# plt.show()
#
# plt.plot(x_train,f_wb)
# plt.show()
# #
# #
# #
# # ##################################################################################
# #
# #
# # def compute_cost(x, y, w, b):
# #     """
# #     Computes the cost function for linear regression.
# #
# #     Args:
# #       x (ndarray (m,)): Data, m examples
# #       y (ndarray (m,)): target values
# #       w,b (scalar)    : model parameters
# #
# #     Returns
# #         total_cost (float): The cost of using w,b as the parameters for linear regression
# #                to fit the data points in x and y
# #     """
# #     # number of training examples
# #     m = x.shape[0]
# #
# cost_sum = 0
# for i in range(m):
#     f_wb = w * x_i[i] + b
#     cost = (f_wb - y_i[i]) ** 2
#     cost_sum = cost_sum + cost
# total_cost = (1 / (2 * m)) * cost_sum
# #
# # #####################################################################################
# # #calculate the partial derivatives
# #
# #
# # def compute_gradient(x, y, w, b):
# #     """
# #     Computes the gradient for linear regression
# #     Args:
# #       x (ndarray (m,)): Data, m examples
# #       y (ndarray (m,)): target values
# #       w,b (scalar)    : model parameters
# #     Returns
# #       dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
# #       dj_db (scalar): The gradient of the cost w.r.t. the parameter b
# #      """
# #
# #     # Number of training examples
# #     m = x.shape[0]
# dj_dw = 0
# dj_db = 0
# #
# for i in range(m):
#     f_wb = w * x_i[i] + b
#     dj_dw_i = (f_wb - y_i[i]) * x_i[i]
#     dj_db_i = f_wb - y_i[i]
#     dj_db += dj_db_i
#     dj_dw += dj_dw_i
# dj_dw = dj_dw / m
# dj_db = dj_db / m
# #
# #     return dj_dw, dj_db
# #
# # ########################################################################################
# # #calculate gradient descent
# #
# #
# # def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):
# #     """
# #     Performs gradient descent to fit w,b. Updates w,b by taking
# #     num_iters gradient steps with learning rate alpha
# #
# #     Args:
# #       x (ndarray (m,))  : Data, m examples
# #       y (ndarray (m,))  : target values
# #       w_in,b_in (scalar): initial values of model parameters
# #       alpha (float):     Learning rate
# #       num_iters (int):   number of iterations to run gradient descent
# #       cost_function:     function to call to produce cost
# #       gradient_function: function to call to produce gradient
# #
# #     Returns:
# #       w (scalar): Updated value of parameter after running gradient descent
# #       b (scalar): Updated value of parameter after running gradient descent
# #       J_history (List): History of cost values
# #       p_history (list): History of parameters [w,b]
# #       """
# #
# #     # An array to store cost J and w's at each iteration primarily for graphing later
# J_history = []
# p_history = []
# b = b_in
# w = w_in
#
# for i in range(num_iters):
#     # Calculate the gradient and update the parameters using gradient_function
#     dj_dw, dj_db = gradient_function(x, y, w, b)
#
#     # Update Parameters using equation (3) above
#     b = b - alpha * dj_db
#     w = w - alpha * dj_dw
#
#     # Save cost J at each iteration
#     if i < 100000:  # prevent resource exhaustion
#         J_history.append(cost_function(x, y, w, b))
#         p_history.append([w, b])
#     # Print cost every at intervals 10 times or as many iterations if < 10
#     if i % math.ceil(num_iters / 10) == 0:
#         print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
#               f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
#               f"w: {w: 0.3e}, b:{b: 0.5e}")
#
# #     return w, b, J_history, p_history  # return w and J,w history for graphing
# # ##########################################################################################
# data =pd.read_csv("data.csv")
# def msp(x,u):
#     return np.mean((x-u)**2)
# x=data.iloc[:,:-1].values
# y=data.iloc[:,-1].values
#
# w=-3.63
# b=1.16
#
# print(x)
# print(y)
#
# alpha =0.0001
# m=float(len(x))
#
# for i in range ((len(x))):
#     f_wb=w*x+b
#     d_w=(-2/m)*np.sum((y-f_wb)*x)
#     d_b= (-2/m)*np.sum((y-f_wb))
#     w=w-alpha*d_w
#     b=b-alpha*d_b
# plt.scatter(x, y, marker='x', c='r')
# # Set the title
# plt.title("Housing Prices")
# # Set the y-axis label
# plt.ylabel('Price (in 1000s of dollars)')
# # Set the x-axis label
# plt.xlabel('Size (1000 sqft)')
# print(msp(x,f_wb))
# plt.plot(x,f_wb)
# plt.show()
# new_data =pd.read_csv("new_data.csv")
# new_x=new_data.iloc[:,-1].values
# def predict(theta,x,b):
#     return (theta*x)+b
# n = predict(w,new_x,b)
# print(n)